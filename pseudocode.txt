INPUT:
- dataSet   : filename where the data is.
- k         : number of clusters
- condition : (TODO)

Main

Parse the arguments set them as configuration // For hadoop is put the in configuration object and in spark is to use sys.argv
centroids = Choose initial centroids
save centroids in memory/file


load_centroids():
  centroids_list = []

  for each centroids:
      centroid = parse the centroid
      add a centroid to the centroids_list

  return centroids_list

while is not converged:
  # SET UP
  centroids = load the centroids from file/memory (load_centroids)

  # MAP
  method map(point):
    point = parse the list of values from string (split to convert in and array of values)

    # decide to what centroid this point belongs to
    distance = INFINITY
    auxiliar_centroid = null

    for each centroid in centroids_list:
      euclidean_distance = find euclidean distance between centroid and point

      if euclidean_distance < distance:
        distance = euclidean_distance
        auxiliar_centroid = centroid

    EMIT(auxiliar_centroid, point)

  # REDUCE
  save the centroids we found in file/memory


